{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTojMDnBn-mI"
   },
   "outputs": [],
   "source": [
    "# This code loads the necessary libraries, installs any required libraries, and imports the necessary functions.\n",
    "# It also sets the MinMaxScaler function to be the scaler variable.\n",
    "\n",
    "#!pip install nibabel\n",
    "#!pip install matplotlib\n",
    "#!pip install tifffile\n",
    "#!pip install -U scikit-learn\n",
    "#!pip install shutil\n",
    "#!pip install split-folders[full]\n",
    "#!pip install pandas==1.5.2  # Install the pandas library with a specific version as pandas 2.0 removes append\n",
    "\n",
    "#!pip install kaggle #Installing the Kaggle library\n",
    "\n",
    "# Setting the Kaggle username and key\n",
    "%env KAGGLE_USERNAME=ihindal\n",
    "%env KAGGLE_KEY=549e8a0e9862683f6f255cb289ece9de\n",
    "\n",
    "import csv\n",
    "import kaggle # Importing the Kaggle library\n",
    "import pandas as pd  # Import the pandas library\n",
    "import splitfolders\n",
    "import numpy as np\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "import subprocess\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import random\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5imtOT1hn-mJ",
    "outputId": "b89c5d9e-3c89-4018-9315-03f3b555f457"
   },
   "outputs": [],
   "source": [
    "# Getting the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Creating a path for the data directory\n",
    "data_path = os.path.join(current_directory, \"data\")\n",
    "\n",
    "# Creating the data directory if it doesn't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Downloading the dataset using the Kaggle library and saving it to the data directory\n",
    "subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"dschettler8845/brats-2021-task1\", \"-p\", data_path])\n",
    "\n",
    "# Defining the path to the downloaded zip file\n",
    "zip_file_path = os.path.join(data_path, \"brats-2021-task1.zip\")\n",
    "\n",
    "# Extracting the contents of the zip file to the data directory\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xrrjhbh_rTmi"
   },
   "outputs": [],
   "source": [
    "# This code extracts the BraTS2021_Training_Data.tar file into the 'dataset' subfolder of the data_path directory. \n",
    "# The tar file contains the training data for the BraTS 2021 competition. \n",
    "# The extracted files are stored in the dataset_path variable. \n",
    "# The data_path variable is updated to point to the 'dataset' subfolder. \n",
    "\n",
    "# Create 'dataset' subfolder inside the data_path directory\n",
    "dataset_path = os.path.join(data_path, \"dataset\")\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Extract the files into the 'dataset' subfolder\n",
    "tar_file_path = os.path.join(data_path, \"BraTS2021_Training_Data.tar\")\n",
    "with tarfile.open(tar_file_path, 'r') as tar_ref:\n",
    "    tar_ref.extractall(dataset_path)\n",
    "\n",
    "# Update the data_path variable to point to the 'dataset' subfolder\n",
    "data_path = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the list of patient directories in the data_path directory\n",
    "patient_dirs = os.listdir(data_path)\n",
    "\n",
    "# Iterating over each patient directory\n",
    "for patient_dir in patient_dirs:\n",
    "\n",
    "    if patient_dir == \".DS_Store\":\n",
    "        os.remove(os.path.join(data_path, patient_dir))\n",
    "        continue\n",
    "\n",
    "    # Delete 90% of the data\n",
    "    if np.random.rand() < 0.9:\n",
    "        # Removing the patient directory and its contents\n",
    "        shutil.rmtree(os.path.join(data_path, patient_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell can be skipped it is used to produce a csv file with the names of the remaining folders in the dataset folder\n",
    "#verify most has been deleted\n",
    "\n",
    "directory = './data/dataset'  # Replace with the path to your desired directory\n",
    "csv_file = 'output.csv'  # Replace with the desired path and filename for the CSV file\n",
    "\n",
    "# Get all the folder names in the directory\n",
    "folder_names = [name for name in os.listdir(directory) if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "# Write folder names to CSV file\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Folder Name'])  # Writes the collumn header\n",
    "    writer.writerows([[name] for name in folder_names])  # Write folder names\n",
    "\n",
    "print('CSV file has been created successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l_-Ymyl9869J",
    "outputId": "4c5a5ca4-25f8-471d-f545-b61070b55303",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a list of all patient directories\n",
    "patient_dirs = os.listdir(data_path)\n",
    "\n",
    "for patient_dir in patient_dirs:\n",
    "    # Skip .DS_Store files. this is a mac file\n",
    "    if patient_dir == \".DS_Store\":\n",
    "        continue\n",
    "    # Construct the path to the patient's data\n",
    "    patient_path = os.path.join(data_path, patient_dir)\n",
    "\n",
    "    # Construct paths to each type of scan for the patient\n",
    "    flair_path = os.path.join(patient_path, patient_dir + '_flair.nii.gz')\n",
    "    t1_path = os.path.join(patient_path, patient_dir + '_t1.nii.gz')\n",
    "    t1ce_path = os.path.join(patient_path, patient_dir + '_t1ce.nii.gz')\n",
    "    t2_path = os.path.join(patient_path, patient_dir + '_t2.nii.gz')\n",
    "    seg_path = os.path.join(patient_path, patient_dir + '_seg.nii.gz')\n",
    "\n",
    "    # Load and process the data as before. this will process and transform each of the imaging modalities.\n",
    "    test_image_flair = nib.load(flair_path).get_fdata()\n",
    "    test_image_flair = scaler.fit_transform(test_image_flair.reshape(-1, test_image_flair.shape[-1])).reshape(test_image_flair.shape)\n",
    "\n",
    "    test_image_t1 = nib.load(t1_path).get_fdata()\n",
    "    test_image_t1 = scaler.fit_transform(test_image_t1.reshape(-1, test_image_t1.shape[-1])).reshape(test_image_t1.shape)\n",
    "\n",
    "    test_image_t1ce = nib.load(t1ce_path).get_fdata()\n",
    "    test_image_t1ce = scaler.fit_transform(test_image_t1ce.reshape(-1, test_image_t1ce.shape[-1])).reshape(test_image_t1ce.shape)\n",
    "\n",
    "    test_image_t2 = nib.load(t2_path).get_fdata()\n",
    "    test_image_t2 = scaler.fit_transform(test_image_t2.reshape(-1, test_image_t2.shape[-1])).reshape(test_image_t2.shape)\n",
    "\n",
    "    test_mask = nib.load(seg_path).get_fdata()\n",
    "    test_mask = test_mask.astype(np.uint8)\n",
    "\n",
    "    #print(np.unique(test_mask))\n",
    "    test_mask[test_mask==4] = 3\n",
    "    #print(np.unique(test_mask))\n",
    "    '''\n",
    "    # Display the images and mask uncomment to see\n",
    "    import random\n",
    "    n_slice=random.randint(0, test_mask.shape[2])\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(231)\n",
    "    plt.imshow(test_image_flair[:,:,n_slice], cmap='gray')\n",
    "    plt.title('Image flair')\n",
    "    plt.subplot(232)\n",
    "    plt.imshow(test_image_t1[:,:,n_slice], cmap='gray')\n",
    "    plt.title('Image t1')\n",
    "    plt.subplot(233)\n",
    "    plt.imshow(test_image_t1ce[:,:,n_slice], cmap='gray')\n",
    "    plt.title('Image t1ce')\n",
    "    plt.subplot(234)\n",
    "    plt.imshow(test_image_t2[:,:,n_slice], cmap='gray')\n",
    "    plt.title('Image t2')\n",
    "    plt.subplot(235)\n",
    "    plt.imshow(test_mask[:,:,n_slice])\n",
    "    plt.title('Mask')\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Combine and crop the images\n",
    "    combined_x = np.stack([test_image_flair, test_image_t1ce, test_image_t2], axis=3)\n",
    "    combined_x = combined_x[56:184, 56:184, 13:141]\n",
    "    test_mask = test_mask[56:184, 56:184, 13:141]\n",
    "    '''\n",
    "    # Display the images and mask uncomment to see\n",
    "    n_slice=random.randint(0, test_mask.shape[2])\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(combined_x[:,:,n_slice, 0], cmap='gray')\n",
    "    plt.title('Image flair')\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(combined_x[:,:,n_slice, 1], cmap='gray')\n",
    "    plt.title('Image t1ce')\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(combined_x[:,:,n_slice, 2], cmap='gray')\n",
    "    plt.title('Image t2')\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(test_mask[:,:,n_slice])\n",
    "    plt.title('Mask')\n",
    "    plt.show()\n",
    "    '''\n",
    "    # Save the combined image as .tif and .npy files in the patient's directory\n",
    "    imsave(os.path.join(patient_path, 'combined255.tif'), combined_x)\n",
    "    np.save(os.path.join(patient_path, 'combined255.npy'), combined_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCEUhTCGn-mL"
   },
   "outputs": [],
   "source": [
    "# List of all the directories containing the different MRI sequences\n",
    "t1_list = sorted(glob.glob('data/dataset/BraTS2021_*/BraTS2021_*_t1.nii.gz'))\n",
    "t2_list = sorted(glob.glob('data/dataset/BraTS2021_*/BraTS2021_*_t2.nii.gz'))\n",
    "t1ce_list = sorted(glob.glob('data/dataset/BraTS2021_*/BraTS2021_*_t1ce.nii.gz'))\n",
    "flair_list = sorted(glob.glob('data/dataset/BraTS2021_*/BraTS2021_*_flair.nii.gz'))\n",
    "mask_list = sorted(glob.glob('data/dataset/BraTS2021_*/BraTS2021_*_seg.nii.gz'))\n",
    "\n",
    "# Defines the list of combinations of images used. \n",
    "combinations = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "\n",
    "\n",
    "# Ensure that the 'input_data_3channels' directory and its subdirectories exist\n",
    "os.makedirs('BraTS2021_TrainingData/input_data_3channels', exist_ok=True)\n",
    "\n",
    "# Create subdirectories for each combination\n",
    "for combo in combinations:\n",
    "    combo_name = \"_\".join(combo)\n",
    "    os.makedirs(f'BraTS2021_TrainingData/input_data_3channels/{combo_name}/images', exist_ok=True)\n",
    "    os.makedirs(f'BraTS2021_TrainingData/input_data_3channels/{combo_name}/masks', exist_ok=True)\n",
    "\n",
    "# Loop over the combinations\n",
    "for combo in combinations:\n",
    "    # Loop over the images\n",
    "    for img in range(len(t2_list)):\n",
    "        print(\"Now preparing image and masks number: \", img)\n",
    "        \n",
    "        temp_images = []\n",
    "        for layer in combo:\n",
    "            if layer == 't2':\n",
    "                temp_image=nib.load(t2_list[img]).get_fdata()\n",
    "            elif layer == 't1':\n",
    "                temp_image=nib.load(t1_list[img]).get_fdata()\n",
    "            elif layer == 't1ce':\n",
    "                temp_image=nib.load(t1ce_list[img]).get_fdata()\n",
    "            elif layer == 'flair':\n",
    "                temp_image=nib.load(flair_list[img]).get_fdata()\n",
    "                \n",
    "            temp_image=scaler.fit_transform(temp_image.reshape(-1, temp_image.shape[-1])).reshape(temp_image.shape)\n",
    "            temp_images.append(temp_image)\n",
    "        \n",
    "        temp_mask=nib.load(mask_list[img]).get_fdata()\n",
    "        temp_mask=temp_mask.astype(np.uint8)\n",
    "        temp_mask[temp_mask==4] = 3  # Reassign mask values 4 to 3\n",
    "\n",
    "        temp_combined_images = np.stack(temp_images, axis=3)\n",
    "        \n",
    "        # Crop to a size to be divisible by 64 so we can later extract 64x64x64 patches removes the outside black area.\n",
    "        temp_combined_images=temp_combined_images[56:184, 56:184, 13:141]\n",
    "        temp_mask = temp_mask[56:184, 56:184, 13:141]\n",
    "        \n",
    "        val, counts = np.unique(temp_mask, return_counts=True)\n",
    "        \n",
    "        if (1 - (counts[0]/counts.sum())) > 0.01:\n",
    "            print(\"Save Me\")\n",
    "            temp_mask= to_categorical(temp_mask, num_classes=4)\n",
    "            combo_name = \"_\".join(combo)\n",
    "            np.save(f'BraTS2021_TrainingData/input_data_3channels/{combo_name}/images/image_{img}.npy', temp_combined_images)\n",
    "            np.save(f'BraTS2021_TrainingData/input_data_3channels/{combo_name}/masks/mask_{img}.npy', temp_mask)\n",
    "            \n",
    "        else:\n",
    "            print(\"I am useless\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5R_C_HLZn-mM",
    "outputId": "5b369d6c-6fc5-4bfc-c278-d8a841486a01"
   },
   "outputs": [],
   "source": [
    "# This code splits the dataset into train and validation sets. It also splits the dataset into different combinations of input channels, and saves the output as a new dataset. \n",
    "combinations = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "\n",
    "output_folder = 'BraTS2021_TrainingData/input_data_128/'\n",
    "\n",
    "for combo in combinations:\n",
    "    combo_name = \"_\".join(combo)\n",
    "    input_folder = f'BraTS2021_TrainingData/input_data_3channels/{combo_name}'\n",
    "    output_combo_folder = f'{output_folder}/{combo_name}'\n",
    "    \n",
    "    splitfolders.ratio(input_folder, output=output_combo_folder, seed=42, ratio=(.75, .25), group_prefix=None) # default values\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESET THE KERNEL HERE. IT PREVENTS AN ERROR BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install segmentation-models-3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd  # Import the pandas library\n",
    "import splitfolders\n",
    "import numpy as np\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imsave\n",
    "import subprocess\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "import random\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5afxj1RFgZH"
   },
   "outputs": [],
   "source": [
    "def load_img(img_dir, img_list):\n",
    "    \"\"\"\n",
    "    Load image data from files.\n",
    "\n",
    "    Args:\n",
    "    - img_dir (str): Directory path where the image files are located.\n",
    "    - img_list (list): List of image filenames.\n",
    "\n",
    "    Returns:\n",
    "    - images (ndarray): NumPy array containing the loaded image data.\n",
    "    \"\"\"\n",
    "\n",
    "    images = []  # A list to store the loaded image data\n",
    "    for i, image_name in enumerate(img_list):\n",
    "        if image_name.split('.')[1] == 'npy':\n",
    "            # Load the image from the .npy file\n",
    "            image = np.load(img_dir + image_name)\n",
    "            images.append(image)\n",
    "    \n",
    "    images = np.array(images)  # Convert the list of images to a NumPy array\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataloader function. loads the images for training and validation.\n",
    "def imageLoader(img_dir, img_list, mask_dir, mask_list, batch_size):\n",
    "    \"\"\"\n",
    "    A generator function that loads image and mask data in batches.\n",
    "\n",
    "    Args:\n",
    "    - img_dir (str): Directory path where the image files are located.\n",
    "    - img_list (list): List of image filenames.\n",
    "    - mask_dir (str): Directory path where the mask files are located.\n",
    "    - mask_list (list): List of mask filenames.\n",
    "    - batch_size (int): Number of samples per batch.\n",
    "\n",
    "    Returns:\n",
    "    - A generator that yields batches of image and mask data.\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(img_list)\n",
    "    while True:\n",
    "        batch_start = 0\n",
    "        batch_end = batch_size\n",
    "        while batch_start < L:\n",
    "            limit = min(batch_end, L)\n",
    "            X = load_img(img_dir, img_list[batch_start:limit])\n",
    "            Y = load_img(mask_dir, mask_list[batch_start:limit])\n",
    "            yield (X, Y)\n",
    "            batch_start += batch_size\n",
    "            batch_end += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "batch_size = 8\n",
    "\n",
    "train_img_datagen_list = []  # List to store image generators for each combination\n",
    "train_img_list_list = []  # List to store image lists for each combination\n",
    "\n",
    "# Loop over the combinations\n",
    "for combo in combinations:\n",
    "    combo_name = \"_\".join(combo)  # Join the combination elements with an underscore\n",
    "    train_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/images/\"\n",
    "    train_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/masks/\"\n",
    "    train_img_list = os.listdir(train_img_dir)  # Get the list of training image filenames\n",
    "    train_mask_list = os.listdir(train_mask_dir)  # Get the list of training mask filenames\n",
    "\n",
    "    # Create an image generator for the current combination\n",
    "    train_img_datagen = imageLoader(train_img_dir, train_img_list, train_mask_dir, train_mask_list, batch_size)\n",
    "\n",
    "    # Append the image generator and image list to the corresponding lists\n",
    "    train_img_datagen_list.append(train_img_datagen)\n",
    "    train_img_list_list.append(train_img_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code displays a random image from the training dataset, as well as the associated mask.\n",
    "# The mask is a 3D array where each slice is a different mask. The mask is a 2D array where each pixel is a different class.\n",
    "\n",
    "img, msk = next(train_img_datagen)  # Get the next batch of images and masks\n",
    "img_num = random.randint(0, img.shape[0] - 1)  # Choose a random image from the batch\n",
    "test_img = img[img_num]  # Select the chosen image\n",
    "test_mask = msk[img_num]  # Select the corresponding mask\n",
    "test_mask = np.argmax(test_mask, axis=3)  # Convert the one-hot encoded mask to a categorical mask\n",
    "n_slice = random.randint(0, test_mask.shape[2])  # Choose a random slice from the mask\n",
    "\n",
    "# Display the images and mask\n",
    "'''\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(221)\n",
    "plt.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.subplot(222)\n",
    "plt.imshow(test_img[:, :, n_slice, 1], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "plt.subplot(223)\n",
    "plt.imshow(test_img[:, :, n_slice, 2], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "plt.subplot(224)\n",
    "plt.imshow(test_mask[:, :, n_slice])\n",
    "plt.title('Mask')\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "combinations = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "# Select a combination for testing\n",
    "combo_name = \"_\".join(combinations[0])  # Change the index to select a different combination\n",
    "\n",
    "train_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/images/\"\n",
    "train_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/masks/\"\n",
    "\n",
    "img_list = os.listdir(train_img_dir)  # Get the list of training image filenames\n",
    "msk_list = os.listdir(train_mask_dir)  # Get the list of training mask filenames\n",
    "\n",
    "num_images = len(img_list)  # Count the number of images\n",
    "\n",
    "img_num = random.randint(0, num_images-1)  # Choose a random image index\n",
    "test_img = np.load(train_img_dir + img_list[img_num])  # Load the chosen image\n",
    "test_mask = np.load(train_mask_dir + msk_list[img_num])  # Load the corresponding mask\n",
    "test_mask = np.argmax(test_mask, axis=3)  # Convert the one-hot encoded mask to a categorical mask\n",
    "\n",
    "'''\n",
    "n_slice = random.randint(0, test_mask.shape[2])  # Choose a random slice from the mask\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "plt.title('Image flair')\n",
    "plt.subplot(222)\n",
    "plt.imshow(test_img[:, :, n_slice, 1], cmap='gray')\n",
    "plt.title('Image t1ce')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(test_img[:,:,n_slice, 2], cmap='gray')\n",
    "plt.title('Image t2')\n",
    "plt.subplot(224)\n",
    "plt.imshow(test_mask[:,:,n_slice])\n",
    "plt.title('Mask')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combinations = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "\n",
    "columns = ['0', '1', '2', '3']  # Define column names for the DataFrame\n",
    "df = pd.DataFrame(columns=columns)  # Create an empty DataFrame with the specified columns\n",
    "\n",
    "# Iterate over the combinations\n",
    "for combo in combinations:\n",
    "    combo_name = \"_\".join(combo)  # Join the combination elements with an underscore\n",
    "    train_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/masks/\"\n",
    "    train_mask_list = sorted(glob.glob(train_mask_dir + '*.npy'))  # Get a sorted list of mask filenames\n",
    "\n",
    "    # Iterate over the mask filenames\n",
    "    for img in range(len(train_mask_list)):\n",
    "        print(img)  # Print the image index\n",
    "        temp_image = np.load(train_mask_list[img])  # Load the mask as a NumPy array\n",
    "        temp_image = np.argmax(temp_image, axis=3)  # Convert the one-hot encoded mask to a categorical mask\n",
    "        val, counts = np.unique(temp_image, return_counts=True)  # Count the occurrences of each label\n",
    "        zipped = zip(columns, counts)  # Zip the column names and counts together\n",
    "        conts_dict = dict(zipped)  # Create a dictionary from the zipped values\n",
    "\n",
    "        df = df.append(conts_dict, ignore_index=True)  # Append the counts dictionary as a new row to the DataFrame\n",
    "\n",
    "# Calculate class weights based on label counts\n",
    "label_0 = df['0'].sum()\n",
    "label_1 = df['1'].sum()\n",
    "label_2 = df['2'].sum()\n",
    "label_3 = df['3'].sum()\n",
    "total_labels = label_0 + label_1 + label_2 + label_3\n",
    "n_classes = 4\n",
    "\n",
    "# Class weights calculation: n_samples / (n_classes * n_samples_for_class)\n",
    "wt0 = round((total_labels / (n_classes * label_0)), 2)  # Calculate class weight for label 0\n",
    "wt1 = round((total_labels / (n_classes * label_1)), 2)  # Calculate class weight for label 1\n",
    "wt2 = round((total_labels / (n_classes * label_2)), 2)  # Calculate class weight for label 2\n",
    "wt3 = round((total_labels / (n_classes * label_3)), 2)  # Calculate class weight for label 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image generators for training and validation\n",
    "\n",
    "combinations = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "\n",
    "# Select a combination for training and validation\n",
    "combo_name = \"_\".join(combinations[0])  # Change the index to select a different combination\n",
    "\n",
    "train_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/images/\"\n",
    "train_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/masks/\"\n",
    "\n",
    "val_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/val/images/\"\n",
    "val_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/val/masks/\"\n",
    "\n",
    "train_img_list = os.listdir(train_img_dir)  # Get the list of training image filenames\n",
    "train_mask_list = os.listdir(train_mask_dir)  # Get the list of training mask filenames\n",
    "\n",
    "val_img_list = os.listdir(val_img_dir)  # Get the list of validation image filenames\n",
    "val_mask_list = os.listdir(val_mask_dir)  # Get the list of validation mask filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Define the batch size for the data generators\n",
    "\n",
    "# Loop over combinations and create data generators\n",
    "for combo in combinations:\n",
    "    combo_name = \"_\".join(combo)  # Join the combination elements with an underscore\n",
    "    train_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/images/\"\n",
    "    train_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/masks/\"\n",
    "    val_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/val/images/\"\n",
    "    val_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/val/masks/\"\n",
    "    \n",
    "    train_img_list = os.listdir(train_img_dir)  # Get the list of training image filenames\n",
    "    train_mask_list = os.listdir(train_mask_dir)  # Get the list of training mask filenames\n",
    "    val_img_list = os.listdir(val_img_dir)  # Get the list of validation image filenames\n",
    "    val_mask_list = os.listdir(val_mask_dir)  # Get the list of validation mask filenames\n",
    "\n",
    "    train_img_datagen = imageLoader(train_img_dir, train_img_list, \n",
    "                                    train_mask_dir, train_mask_list, batch_size)  # Create a data generator for training images\n",
    "\n",
    "    val_img_datagen = imageLoader(val_img_dir, val_img_list, \n",
    "                                    val_mask_dir, val_mask_list, batch_size)  # Create a data generator for validation images\n",
    "\n",
    "    # Verify generator by getting a batch of images and masks\n",
    "    img, msk = train_img_datagen.__next__()  # Get the next batch of images and masks\n",
    "\n",
    "    img_num = random.randint(0,img.shape[0]-1)  # Choose a random image from the batch\n",
    "    test_img = img[img_num]  # Select the chosen image\n",
    "    test_mask = msk[img_num]  # Select the corresponding mask\n",
    "    test_mask = np.argmax(test_mask, axis=3)  # Convert the one-hot encoded mask to a categorical mask\n",
    "\n",
    "    '''\n",
    "    n_slice = random.randint(0, test_mask.shape[2])  # Choose a random slice from the mask\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(221)\n",
    "    plt.imshow(test_img[:,:,n_slice, 0], cmap='gray')\n",
    "    plt.title('Image flair')\n",
    "    plt.subplot(222)\n",
    "    plt.imshow(test_img[:,:,n_slice, 1], cmap='gray')\n",
    "    plt.title('Image t1ce')\n",
    "    plt.subplot(223)\n",
    "    plt.imshow(test_img[:,:,n_slice, 2], cmap='gray')\n",
    "    plt.title('Image t2')\n",
    "    plt.subplot(224)\n",
    "    plt.imshow(test_mask[:,:,n_slice])\n",
    "    plt.title('Mask')\n",
    "    plt.show()\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install segmentation-models-3D\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import segmentation_models_3D as sm\n",
    "\n",
    "# Define the metrics to evaluate the model\n",
    "metrics = ['accuracy', sm.metrics.IOUScore(threshold=0.5)]\n",
    "\n",
    "# Define the weights for the classes\n",
    "wt0, wt1, wt2, wt3 = 0.25, 0.25, 0.25, 0.25  \n",
    "\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=np.array([wt0, wt1, wt2, wt3]))  # Define the Dice loss with class weights\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()  # Define the Categorical Focal loss\n",
    "total_loss = dice_loss + (1 * focal_loss)  # Combine the losses\n",
    "# Define the learning rate\n",
    "LR = 0.0001  \n",
    "\n",
    "# Define the optimizer\n",
    "optim = keras.optimizers.Adam(LR)\n",
    "\n",
    "\n",
    "\n",
    "# List of combinations for training\n",
    "combo_list = [('t1',), ('t1ce',), ('t2',), ('flair',), ('t1', 't1ce'), ('t1', 't2'), ('t1', 'flair'), ('t1ce', 't2'), ('t1ce', 'flair'), ('t2', 'flair'), ('t1', 't1ce', 't2'), ('t1', 't1ce', 'flair'), ('t1', 't2', 'flair'), ('t1ce', 't2', 'flair'), ('t1', 't1ce', 't2', 'flair')]\n",
    "\n",
    "for combo_name in combo_list:\n",
    "    train_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/images/\"\n",
    "    train_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/train/masks/\"\n",
    "    val_img_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/val/images/\"\n",
    "    val_mask_dir = f\"BraTS2021_TrainingData/input_data_128/{combo_name}/val/masks/\"\n",
    "\n",
    "    train_img_list = os.listdir(train_img_dir)  # Get the list of training image filenames\n",
    "    train_mask_list = os.listdir(train_mask_dir)  # Get the list of training mask filenames\n",
    "    val_img_list = os.listdir(val_img_dir)  # Get the list of validation image filenames\n",
    "    val_mask_list = os.listdir(val_mask_dir)  # Get the list of validation mask filenames\n",
    "\n",
    "    steps_per_epoch = len(train_img_list) // batch_size  # Calculate the number of steps per epoch\n",
    "    val_steps_per_epoch = len(val_img_list) // batch_size  # Calculate the number of validation steps per epoch\n",
    "\n",
    "    # Import the simple_3d_unet_model\n",
    "    from simple_3d_unet import simple_unet_model  \n",
    "\n",
    "    # Create the UNet model with the specified dimensions and number of channels\n",
    "    model = simple_unet_model(IMG_HEIGHT=128, IMG_WIDTH=128, IMG_DEPTH=128, IMG_CHANNELS=3, num_classes=4)\n",
    "\n",
    "    # Compile the model with the loss and metrics\n",
    "    model.compile(optimizer=optim, loss=total_loss, metrics=metrics)\n",
    "\n",
    "    print(model.summary())  # Print the model summary\n",
    "\n",
    "    print(model.input_shape)  # Print the input shape of the model\n",
    "    print(model.output_shape)  # Print the output shape of the model\n",
    "\n",
    "    # Create a checkpoint to save the model with the best accuracy\n",
    "    best_model_path = f'brats_3d_{combo_name}_best.hdf5'\n",
    "    best_model_checkpoint = ModelCheckpoint(best_model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(train_img_datagen,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_data=val_img_datagen,\n",
    "                        validation_steps=val_steps_per_epoch,\n",
    "                        callbacks=[best_model_checkpoint]) # Add the checkpoint callback here\n",
    "\n",
    "        # Define the file path for saving the logs\n",
    "    log_file_path = f'training_logs_{combo_name}.txt'\n",
    "\n",
    "    # Open the log file\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        # Write the headers to the log file\n",
    "        log_file.write('Epoch\\tTrain_Loss\\tTrain_Accuracy\\tVal_Loss\\tVal_Accuracy\\n')\n",
    "\n",
    "        # Loop over each epoch\n",
    "        for epoch in range(len(history.history['loss'])):\n",
    "            # Extract the values for this epoch\n",
    "            train_loss = history.history['loss'][epoch]\n",
    "            train_acc = history.history['accuracy'][epoch]\n",
    "            val_loss = history.history['val_loss'][epoch]\n",
    "            val_acc = history.history['val_accuracy'][epoch]\n",
    "\n",
    "            # Write the values for this epoch to the log file\n",
    "            log_file.write(f'{epoch+1}\\t{train_loss}\\t{train_acc}\\t{val_loss}\\t{val_acc}\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = f'brats_3d_{combo_name}_final.hdf5'\n",
    "    model.save(final_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Import the os module\n",
    "\n",
    "subfolder_name = 'data'  # Specify the name of the subfolder to delete\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the path to the subfolder\n",
    "subfolder_path = os.path.join(current_directory, subfolder_name)\n",
    "\n",
    "# Delete the subfolder and its contents recursively using shutil.rmtree()\n",
    "shutil.rmtree(subfolder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation IoU and loss at each epoch\n",
    "\n",
    "loss = history.history['loss']  # Get the training loss history\n",
    "val_loss = history.history['val_loss']  # Get the validation loss history\n",
    "epochs = range(1, len(loss) + 1)  # Generate the x-axis values for epochs\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']  # Get the training accuracy history\n",
    "val_acc = history.history['val_accuracy']  # Get the validation accuracy history\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(epochs, acc, 'y', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model  # Import the load_model function from keras.models\n",
    "\n",
    "# Load the model for prediction or continue training\n",
    "my_model = load_model('saved_models/brats_3d_100epochs_simple_unet_weighted_dice.hdf5', \n",
    "                      custom_objects={'dice_loss_plus_1focal_loss': total_loss,\n",
    "                                      'iou_score':sm.metrics.IOUScore(threshold=0.5)})\n",
    "# Load the saved model using load_model() and specify custom_objects for custom losses and metrics\n",
    "\n",
    "# Now all set to continue the training process.\n",
    "history2 = my_model.fit(train_img_datagen,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        epochs=1,\n",
    "                        verbose=1,\n",
    "                        validation_data=val_img_datagen,\n",
    "                        validation_steps=val_steps_per_epoch)\n",
    "# Continue training the model for 1 additional epoch using the loaded model and data generators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For predictions, you do not need to compile the model, so ...\n",
    "my_model = load_model('saved_models/brats_3d_100epochs_simple_unet_weighted_dice.hdf5', \n",
    "                      compile=False)\n",
    "# Load the saved model without compiling it\n",
    "\n",
    "# Verify IoU on a batch of images from the test dataset\n",
    "# Using built-in Keras function for IoU\n",
    "# Only works on TensorFlow > 2.0\n",
    "from keras.metrics import MeanIoU  # Import the MeanIoU metric from keras.metrics\n",
    "\n",
    "batch_size = 8  # Check IoU for a batch of images\n",
    "test_img_datagen = imageLoader(val_img_dir, val_img_list, \n",
    "                                val_mask_dir, val_mask_list, batch_size)\n",
    "# Create a data generator for test images and masks\n",
    "\n",
    "test_image_batch, test_mask_batch = test_img_datagen.__next__()\n",
    "# Generate a batch of test images and masks\n",
    "\n",
    "test_mask_batch_argmax = np.argmax(test_mask_batch, axis=4)\n",
    "test_pred_batch = my_model.predict(test_image_batch)\n",
    "test_pred_batch_argmax = np.argmax(test_pred_batch, axis=4)\n",
    "# Perform predictions on the test image batch and calculate the predicted mask\n",
    "\n",
    "n_classes = 4\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(test_pred_batch_argmax, test_mask_batch_argmax)\n",
    "# Calculate the mean IoU using the predicted masks and the ground truth masks\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "# Predict on a few test images, one at a time\n",
    "# Try images:\n",
    "img_num = 82\n",
    "\n",
    "test_img = np.load(\"BraTS2021_TrainingData/input_data_128/val/images/image_\"+str(img_num)+\".npy\")\n",
    "\n",
    "test_mask = np.load(\"BraTS2021_TrainingData/input_data_128/val/masks/mask_\"+str(img_num)+\".npy\")\n",
    "test_mask_argmax = np.argmax(test_mask, axis=3)\n",
    "\n",
    "test_img_input = np.expand_dims(test_img, axis=0)\n",
    "test_prediction = my_model.predict(test_img_input)\n",
    "test_prediction_argmax = np.argmax(test_prediction, axis=4)[0,:,:,:]\n",
    "# Load a specific test image and its corresponding mask\n",
    "# Perform prediction on the test image and calculate the predicted mask\n",
    "\n",
    "# Plot individual slices from test predictions for verification\n",
    "from matplotlib import pyplot as plt  # Import pyplot from matplotlib\n",
    "import random\n",
    "\n",
    "n_slice = 55  # Select a specific slice to plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,n_slice,1], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(test_mask_argmax[:,:,n_slice])\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(test_prediction_argmax[:,:, n_slice])\n",
    "plt.show()\n",
    "# Plot the test image, ground truth mask, and predicted mask for visualization\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
