{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA T1000 8GB, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import tarfile\n",
    "import random\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "import subprocess\n",
    "\n",
    "# Enable mixed precision training\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, modalities, fraction=1.0):\n",
    "    data_list = []\n",
    "    mask_list = []\n",
    "\n",
    "    patient_folders = os.listdir(data_path)\n",
    "    random.shuffle(patient_folders)\n",
    "    num_patients = int(len(patient_folders) * fraction)\n",
    "    selected_patients = patient_folders[:num_patients]\n",
    "\n",
    "    for patient_folder in selected_patients:\n",
    "        patient_path = os.path.join(data_path, patient_folder)\n",
    "\n",
    "        image_data = []\n",
    "        for modality in modalities:\n",
    "            modality_file = os.path.join(patient_path, f\"{patient_folder}_{modality}.nii.gz\")\n",
    "            modality_data = nib.load(modality_file).get_fdata(dtype=np.float32)  # Add dtype=np.float32\n",
    "            image_data.append(modality_data)\n",
    "\n",
    "        mask_file = os.path.join(patient_path, f\"{patient_folder}_seg.nii.gz\")\n",
    "        mask_data = nib.load(mask_file).get_fdata()\n",
    "\n",
    "        data_list.append(np.stack(image_data, axis=-1))\n",
    "        mask_list.append(mask_data)\n",
    "\n",
    "    return np.array(data_list), np.array(mask_list)\n",
    "\n",
    "# Create segmentation model\n",
    "def create_segmentation_model(input_shape, base_model_name):\n",
    "    input_shape = (*input_shape, len(modalities))  # Add this line to update input_shape\n",
    "    base_model = tf.keras.applications.ResNet50V2(input_shape=input_shape, include_top=False, weights=None)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = UpSampling2D(size=(2, 2))(x)\n",
    "    x = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_modality_combinations(modalities):\n",
    "    combinations_list = []\n",
    "    for i in range(1, len(modalities) + 1):\n",
    "        for subset in combinations(modalities, i):\n",
    "            combinations_list.append(list(subset))\n",
    "    return combinations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "data_path = os.path.join(current_directory, \"data\")\n",
    "os.makedirs(data_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "\n",
    "%env KAGGLE_USERNAME=ihindal\n",
    "%env KAGGLE_KEY=549e8a0e9862683f6f255cb289ece9de\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['kaggle', 'datasets', 'download', '-d', 'dschettler8845/brats-2021-task1', '-p', 'C:\\\\Users\\\\c1606436\\\\Yr4_MIP\\\\data'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"kaggle\", \"datasets\", \"download\", \"-d\", \"dschettler8845/brats-2021-task1\", \"-p\", data_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "zip_file_path = os.path.join(data_path, \"brats-2021-task1.zip\")\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'dataset' subfolder inside the data_path directory\n",
    "dataset_path = os.path.join(data_path, \"dataset\")\n",
    "os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "# Extract the files into the 'dataset' subfolder\n",
    "tar_file_path = os.path.join(data_path, \"BraTS2021_Training_Data.tar\")\n",
    "with tarfile.open(tar_file_path, 'r') as tar_ref:\n",
    "    tar_ref.extractall(dataset_path)\n",
    "\n",
    "# Update the data_path variable to point to the 'dataset' subfolder\n",
    "data_path = dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with modalities: ['T1']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 34.1 MiB for an array with shape (240, 240, 155) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with modalities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodalities\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m---> 15\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n\u001b[0;32m     18\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(data_path, modalities, fraction)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modality \u001b[38;5;129;01min\u001b[39;00m modalities:\n\u001b[0;32m     15\u001b[0m     modality_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patient_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     modality_data \u001b[38;5;241m=\u001b[39m \u001b[43mnib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodality_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_fdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Add dtype=np.float32\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     image_data\u001b[38;5;241m.\u001b[39mappend(modality_data)\n\u001b[0;32m     19\u001b[0m mask_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(patient_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seg.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\nibabel\\dataobj_images.py:368\u001b[0m, in \u001b[0;36mDataobjImage.get_fdata\u001b[1;34m(self, caching, dtype)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Always return requested data type\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# For array proxies, will attempt to confine data array to dtype\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# during scaling\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m caching \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fdata_cache \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\nibabel\\arrayproxy.py:426\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 426\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\nibabel\\arrayproxy.py:395\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    393\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_unscaled(slicer\u001b[38;5;241m=\u001b[39mslicer), scl_slope, scl_inter)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 395\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scaled\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 34.1 MiB for an array with shape (240, 240, 155) and data type float32"
     ]
    }
   ],
   "source": [
    "# Define the parameters\n",
    "all_modalities = ['t1', 't1ce', 't2', 'flair']\n",
    "modality_combinations = generate_modality_combinations(all_modalities)\n",
    "base_model_name = \"ResNet50V2\"\n",
    "\n",
    "# Increase batch size per GPU\n",
    "batch_size_per_gpu = 4\n",
    "num_gpus = 1\n",
    "total_batch_size = batch_size_per_gpu * num_gpus\n",
    "\n",
    "for modalities in modality_combinations:\n",
    "    print(f\"Training with modalities: {modalities}\")\n",
    "\n",
    "    fraction = 0.1\n",
    "    X, y = load_data(data_path, modalities, fraction)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = create_segmentation_model(input_shape=X_train.shape[1:], base_model_name=base_model_name)\n",
    "\n",
    "    # Scale the learning rate\n",
    "    lr = 1e-3 * (total_batch_size / 16)\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Set up the TensorBoard callback\n",
    "    tensorboard_callback = TensorBoard(\n",
    "        log_dir=f'./logs/{base_model_name}_{\"_\".join(modalities)}',\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch',\n",
    "        profile_batch=2,\n",
    "        embeddings_freq=1\n",
    "    )\n",
    "\n",
    "    fraction = 0.1\n",
    "    X, y = load_data(data_path, modalities, fraction)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = create_segmentation_model(input_shape=X_train.shape[1:], base_model_name=base_model_name)\n",
    "\n",
    "    # Scale the learning rate\n",
    "    lr = 1e-3 * (total_batch_size / 16)\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
